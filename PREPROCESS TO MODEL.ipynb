{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2f3c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Summaries Train Data:\n",
      "                                    text   content   wording  \\\n",
      "0    0.205682506482641 0.380537638762288  0.205683  0.380538   \n",
      "1   -0.548304076980462 0.506755353548534 -0.548304  0.506755   \n",
      "2      3.12892846350062 4.23122555224945  3.128928  4.231226   \n",
      "3  -0.210613934166593 -0.471414826967448 -0.210614 -0.471415   \n",
      "4      3.27289414977436 3.21975651022738  3.272894  3.219757   \n",
      "\n",
      "                   preprocessed_text                                tokens  \\\n",
      "0  0205682506482641 0380537638762288  [0205682506482641, 0380537638762288]   \n",
      "1  0548304076980462 0506755353548534  [0548304076980462, 0506755353548534]   \n",
      "2    312892846350062 423122555224945    [312892846350062, 423122555224945]   \n",
      "3  0210613934166593 0471414826967448  [0210613934166593, 0471414826967448]   \n",
      "4    327289414977436 321975651022738    [327289414977436, 321975651022738]   \n",
      "\n",
      "                         stemmed_tokens                     lemmatized_tokens  \\\n",
      "0  [0205682506482641, 0380537638762288]  [0205682506482641, 0380537638762288]   \n",
      "1  [0548304076980462, 0506755353548534]  [0548304076980462, 0506755353548534]   \n",
      "2    [312892846350062, 423122555224945]    [312892846350062, 423122555224945]   \n",
      "3  [0210613934166593, 0471414826967448]  [0210613934166593, 0471414826967448]   \n",
      "4    [327289414977436, 321975651022738]    [327289414977436, 321975651022738]   \n",
      "\n",
      "                                           pos_tags  \n",
      "0  [(0205682506482641, CD), (0380537638762288, CD)]  \n",
      "1  [(0548304076980462, CD), (0506755353548534, CD)]  \n",
      "2    [(312892846350062, CD), (423122555224945, CD)]  \n",
      "3  [(0210613934166593, CD), (0471414826967448, CD)]  \n",
      "4    [(327289414977436, CD), (321975651022738, CD)]  \n",
      "\n",
      "Preprocessed Prompts Train Data:\n",
      "                                         prompt_text  \\\n",
      "0  Summarize at least 3 elements of an ideal trag...   \n",
      "1  In complete sentences, summarize the structure...   \n",
      "2  Summarize how the Third Wave developed over su...   \n",
      "3  Summarize the various ways the factory would u...   \n",
      "\n",
      "                                     prompt_question  \\\n",
      "0  Summarize at least 3 elements of an ideal trag...   \n",
      "1  In complete sentences, summarize the structure...   \n",
      "2  Summarize how the Third Wave developed over su...   \n",
      "3  Summarize the various ways the factory would u...   \n",
      "\n",
      "                prompt_title  \\\n",
      "0                 On Tragedy   \n",
      "1  Egyptian Social Structure   \n",
      "2             The Third Wave   \n",
      "3    Excerpt from The Jungle   \n",
      "\n",
      "                            preprocessed_prompt_text  \\\n",
      "0  summarize least 3 elements ideal tragedy descr...   \n",
      "1  complete sentences summarize structure ancient...   \n",
      "2  summarize third wave developed short period ti...   \n",
      "3  summarize various ways factory would use cover...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [summarize, least, 3, elements, ideal, tragedy...   \n",
      "1  [complete, sentences, summarize, structure, an...   \n",
      "2  [summarize, third, wave, developed, short, per...   \n",
      "3  [summarize, various, ways, factory, would, use...   \n",
      "\n",
      "                                      stemmed_tokens  \\\n",
      "0  [summar, least, 3, element, ideal, tragedi, de...   \n",
      "1  [complet, sentenc, summar, structur, ancient, ...   \n",
      "2  [summar, third, wave, develop, short, period, ...   \n",
      "3  [summar, variou, way, factori, would, use, cov...   \n",
      "\n",
      "                                   lemmatized_tokens  \\\n",
      "0  [summarize, least, 3, element, ideal, tragedy,...   \n",
      "1  [complete, sentence, summarize, structure, anc...   \n",
      "2  [summarize, third, wave, developed, short, per...   \n",
      "3  [summarize, various, way, factory, would, use,...   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(summarize, VB), (least, JJS), (3, CD), (elem...  \n",
      "1  [(complete, JJ), (sentences, NNS), (summarize,...  \n",
      "2  [(summarize, VB), (third, JJ), (wave, NN), (de...  \n",
      "3  [(summarize, VB), (various, JJ), (ways, NNS), ...  \n"
     ]
    }
   ],
   "source": [
    "#preprocess step.....\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#DATASET\n",
    "summaries_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_train.csv\"\n",
    "summaries_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_test.csv\"\n",
    "prompts_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_train.csv\"\n",
    "prompts_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_test.csv\"\n",
    "\n",
    "summaries_train_df = pd.read_csv(summaries_train_path)\n",
    "summaries_test_df = pd.read_csv(summaries_test_path)\n",
    "prompts_train_df = pd.read_csv(prompts_train_path)\n",
    "prompts_test_df = pd.read_csv(prompts_test_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'content' and 'wording' COLUMNS ARE CONVERTED TO STRING\n",
    "summaries_train_df['content'] = summaries_train_df['content'].astype(str)\n",
    "summaries_train_df['wording'] = summaries_train_df['wording'].astype(str)\n",
    "\n",
    "#COMBINE TEXT DATA FROM SUMMARAISE AND PROMPTS DATAFRANE\n",
    "summaries_train_df['text'] = summaries_train_df['content'] + \" \" + summaries_train_df['wording']\n",
    "prompts_train_df['prompt_text'] = prompts_train_df['prompt_question'] + \" \" + prompts_train_df['prompt_title'] + \" \" + prompts_train_df['prompt_text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # CONVERT TO LOWERCASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # PREMOVE SPECIAL CHERECTER AND PUNCTUATION\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOPWORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FOR  summaries_train_df\n",
    "summaries_train_df['preprocessed_text'] = summaries_train_df['text'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_content'] = summaries_train_df['content'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_wording'] = summaries_train_df['wording'].apply(preprocess_text)\n",
    "\n",
    "#  PREPROCESSING FOR prompts_train_df\n",
    "prompts_train_df['preprocessed_prompt_text'] = prompts_train_df['prompt_text'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_question'] = prompts_train_df['prompt_question'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_title'] = prompts_train_df['prompt_title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# TOKENIZATION, STEMMING, AND LEMMATIZATION\n",
    "summaries_train_df['tokens'] = summaries_train_df['preprocessed_text'].apply(word_tokenize)\n",
    "summaries_train_df['stemmed_tokens'] = summaries_train_df['tokens'].apply(apply_stemming)\n",
    "summaries_train_df['lemmatized_tokens'] = summaries_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "prompts_train_df['tokens'] = prompts_train_df['preprocessed_prompt_text'].apply(word_tokenize)\n",
    "prompts_train_df['stemmed_tokens'] = prompts_train_df['tokens'].apply(apply_stemming)\n",
    "prompts_train_df['lemmatized_tokens'] = prompts_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT 'content' and 'wording' COLUMNS TO NUMERIC\n",
    "summaries_train_df['content'] = pd.to_numeric(summaries_train_df['content'], errors='coerce')\n",
    "summaries_train_df['wording'] = pd.to_numeric(summaries_train_df['wording'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# DROP NAN VALUES IN  'content' and 'wording' columns\n",
    "summaries_train_df = summaries_train_df.dropna(subset=['content', 'wording'])\n",
    "\n",
    "\n",
    "\n",
    "# PART OF SPEECH TAGGING\n",
    "\n",
    "\n",
    "def apply_pos_tagging(tokens):\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "summaries_train_df['pos_tags'] = summaries_train_df['tokens'].apply(apply_pos_tagging)\n",
    "prompts_train_df['pos_tags'] = prompts_train_df['tokens'].apply(apply_pos_tagging)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Preprocessed Summaries Train Data:\")\n",
    "print(summaries_train_df[['text','content','wording', 'preprocessed_text', 'tokens', 'stemmed_tokens', 'lemmatized_tokens', 'pos_tags']].head())\n",
    "\n",
    "print(\"\\nPreprocessed Prompts Train Data:\")\n",
    "print(prompts_train_df[['prompt_text','prompt_question','prompt_title', 'preprocessed_prompt_text', 'tokens', 'stemmed_tokens', 'lemmatized_tokens', 'pos_tags']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22c0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b5b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0579d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Summaries Train Data with TF-IDF:\n",
      "                                    text                  preprocessed_text  \\\n",
      "0    0.205682506482641 0.380537638762288  0205682506482641 0380537638762288   \n",
      "1   -0.548304076980462 0.506755353548534  0548304076980462 0506755353548534   \n",
      "2      3.12892846350062 4.23122555224945    312892846350062 423122555224945   \n",
      "3  -0.210613934166593 -0.471414826967448  0210613934166593 0471414826967448   \n",
      "4      3.27289414977436 3.21975651022738    327289414977436 321975651022738   \n",
      "\n",
      "                                 tokens                        stemmed_tokens  \\\n",
      "0  [0205682506482641, 0380537638762288]  [0205682506482641, 0380537638762288]   \n",
      "1  [0548304076980462, 0506755353548534]  [0548304076980462, 0506755353548534]   \n",
      "2    [312892846350062, 423122555224945]    [312892846350062, 423122555224945]   \n",
      "3  [0210613934166593, 0471414826967448]  [0210613934166593, 0471414826967448]   \n",
      "4    [327289414977436, 321975651022738]    [327289414977436, 321975651022738]   \n",
      "\n",
      "                      lemmatized_tokens  \\\n",
      "0  [0205682506482641, 0380537638762288]   \n",
      "1  [0548304076980462, 0506755353548534]   \n",
      "2    [312892846350062, 423122555224945]   \n",
      "3  [0210613934166593, 0471414826967448]   \n",
      "4    [327289414977436, 321975651022738]   \n",
      "\n",
      "                                           pos_tags  \\\n",
      "0  [(0205682506482641, CD), (0380537638762288, CD)]   \n",
      "1  [(0548304076980462, CD), (0506755353548534, CD)]   \n",
      "2    [(312892846350062, CD), (423122555224945, CD)]   \n",
      "3  [(0210613934166593, CD), (0471414826967448, CD)]   \n",
      "4    [(327289414977436, CD), (321975651022738, CD)]   \n",
      "\n",
      "                                        tfidf_matrix  \n",
      "0    (0, 1)\\t0.7071067811865475\\n  (0, 0)\\t0.7071...  \n",
      "1    (0, 0)\\t0.7071067811865475\\n  (0, 1)\\t0.7071...  \n",
      "2    (0, 1)\\t0.7071067811865475\\n  (0, 0)\\t0.7071...  \n",
      "3    (0, 1)\\t0.7071067811865475\\n  (0, 0)\\t0.7071...  \n",
      "4    (0, 0)\\t0.7071067811865475\\n  (0, 1)\\t0.7071...  \n",
      "\n",
      "Preprocessed Prompts Train Data with TF-IDF:\n",
      "                                         prompt_text  \\\n",
      "0  Summarize at least 3 elements of an ideal trag...   \n",
      "1  In complete sentences, summarize the structure...   \n",
      "2  Summarize how the Third Wave developed over su...   \n",
      "3  Summarize the various ways the factory would u...   \n",
      "\n",
      "                            preprocessed_prompt_text  \\\n",
      "0  summarize least 3 elements ideal tragedy descr...   \n",
      "1  complete sentences summarize structure ancient...   \n",
      "2  summarize third wave developed short period ti...   \n",
      "3  summarize various ways factory would use cover...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [summarize, least, 3, elements, ideal, tragedy...   \n",
      "1  [complete, sentences, summarize, structure, an...   \n",
      "2  [summarize, third, wave, developed, short, per...   \n",
      "3  [summarize, various, ways, factory, would, use...   \n",
      "\n",
      "                                      stemmed_tokens  \\\n",
      "0  [summar, least, 3, element, ideal, tragedi, de...   \n",
      "1  [complet, sentenc, summar, structur, ancient, ...   \n",
      "2  [summar, third, wave, develop, short, period, ...   \n",
      "3  [summar, variou, way, factori, would, use, cov...   \n",
      "\n",
      "                                   lemmatized_tokens  \\\n",
      "0  [summarize, least, 3, element, ideal, tragedy,...   \n",
      "1  [complete, sentence, summarize, structure, anc...   \n",
      "2  [summarize, third, wave, developed, short, per...   \n",
      "3  [summarize, various, way, factory, would, use,...   \n",
      "\n",
      "                                            pos_tags  \\\n",
      "0  [(summarize, VB), (least, JJS), (3, CD), (elem...   \n",
      "1  [(complete, JJ), (sentences, NNS), (summarize,...   \n",
      "2  [(summarize, VB), (third, JJ), (wave, NN), (de...   \n",
      "3  [(summarize, VB), (various, JJ), (ways, NNS), ...   \n",
      "\n",
      "                                        tfidf_matrix  \n",
      "0    (0, 157)\\t0.04109974682633932\\n  (0, 158)\\t0...  \n",
      "1    (0, 112)\\t0.03533326266687867\\n  (0, 157)\\t0...  \n",
      "2    (0, 175)\\t0.03002854067691021\\n  (0, 114)\\t0...  \n",
      "3    (0, 200)\\t0.024296476537493127\\n  (0, 39)\\t0...  \n"
     ]
    }
   ],
   "source": [
    "# perform tf-idf\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "#DATASET\n",
    "summaries_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_train.csv\"\n",
    "summaries_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_test.csv\"\n",
    "prompts_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_train.csv\"\n",
    "prompts_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_test.csv\"\n",
    "\n",
    "summaries_train_df = pd.read_csv(summaries_train_path)\n",
    "summaries_test_df = pd.read_csv(summaries_test_path)\n",
    "prompts_train_df = pd.read_csv(prompts_train_path)\n",
    "prompts_test_df = pd.read_csv(prompts_test_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'content' and 'wording' COLUMNS ARE CONVERTED TO STRING\n",
    "summaries_train_df['content'] = summaries_train_df['content'].astype(str)\n",
    "summaries_train_df['wording'] = summaries_train_df['wording'].astype(str)\n",
    "\n",
    "#COMBINE TEXT DATA FROM SUMMARAISE AND PROMPTS DATAFRANE\n",
    "summaries_train_df['text'] = summaries_train_df['content'] + \" \" + summaries_train_df['wording']\n",
    "prompts_train_df['prompt_text'] = prompts_train_df['prompt_question'] + \" \" + prompts_train_df['prompt_title'] + \" \" + prompts_train_df['prompt_text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    \n",
    "    # CONVERT TO LOWERCASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # PREMOVE SPECIAL CHERECTER AND PUNCTUATION\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOPWORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FOR  summaries_train_df\n",
    "summaries_train_df['preprocessed_text'] = summaries_train_df['text'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_content'] = summaries_train_df['content'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_wording'] = summaries_train_df['wording'].apply(preprocess_text)\n",
    "\n",
    "# PREPROCESSING FOR prompts_train_df\n",
    "prompts_train_df['preprocessed_prompt_text'] = prompts_train_df['prompt_text'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_question'] = prompts_train_df['prompt_question'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_title'] = prompts_train_df['prompt_title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# TOKENIZATION, STEMMING, AND LEMMATIZATION\n",
    "summaries_train_df['tokens'] = summaries_train_df['preprocessed_text'].apply(word_tokenize)\n",
    "summaries_train_df['stemmed_tokens'] = summaries_train_df['tokens'].apply(apply_stemming)\n",
    "summaries_train_df['lemmatized_tokens'] = summaries_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "prompts_train_df['tokens'] = prompts_train_df['preprocessed_prompt_text'].apply(word_tokenize)\n",
    "prompts_train_df['stemmed_tokens'] = prompts_train_df['tokens'].apply(apply_stemming)\n",
    "prompts_train_df['lemmatized_tokens'] = prompts_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "# PART OF SPEECH TAGGING\n",
    "def apply_pos_tagging(tokens):\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "summaries_train_df['pos_tags'] = summaries_train_df['tokens'].apply(apply_pos_tagging)\n",
    "prompts_train_df['pos_tags'] = prompts_train_df['tokens'].apply(apply_pos_tagging)\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "def apply_tfidf(tokens):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(tokens)])\n",
    "    return tfidf_matrix\n",
    "\n",
    "summaries_train_df['tfidf_matrix'] = summaries_train_df['lemmatized_tokens'].apply(apply_tfidf)\n",
    "prompts_train_df['tfidf_matrix'] = prompts_train_df['lemmatized_tokens'].apply(apply_tfidf)\n",
    "\n",
    "print(\"Preprocessed Summaries Train Data with TF-IDF:\")\n",
    "print(summaries_train_df[['text', 'preprocessed_text', 'tokens', 'stemmed_tokens', 'lemmatized_tokens', 'pos_tags', 'tfidf_matrix']].head())\n",
    "\n",
    "print(\"\\nPreprocessed Prompts Train Data with TF-IDF:\")\n",
    "print(prompts_train_df[['prompt_text', 'preprocessed_prompt_text', 'tokens', 'stemmed_tokens', 'lemmatized_tokens', 'pos_tags', 'tfidf_matrix']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7161e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78daa7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6189811584089323\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "-0.0015290909730967       0.00      0.00      0.00         1\n",
      "-0.0016108985178951       0.00      0.00      0.00         1\n",
      "-0.0025719695585915       1.00      1.00      1.00         2\n",
      "-0.0052242590109083       1.00      1.00      1.00         3\n",
      "-0.0091077501105194       0.00      0.00      0.00         3\n",
      "-0.0283739794720577       0.00      0.00      0.00         1\n",
      "-0.0319873399650709       0.00      0.00      0.00         1\n",
      "-0.0425161740436652       1.00      1.00      1.00        10\n",
      "-0.0444775230618836       0.00      0.00      0.00         1\n",
      "  -0.04543859410258       1.00      1.00      1.00        34\n",
      "-0.0463996651432763       0.00      0.00      0.00         1\n",
      "-0.0493220852021911       0.00      0.00      0.00         1\n",
      "-0.0522445052611057       0.00      0.00      0.00         2\n",
      "-0.0529354456952042       0.00      0.00      0.00         1\n",
      "-0.0568189367948152       0.00      0.00      0.00         1\n",
      "-0.0634642797737986       0.00      0.00      0.00         1\n",
      "-0.0685883145637294       0.00      0.00      0.00         1\n",
      "-0.0751240951156572       0.00      0.00      0.00         1\n",
      "-0.0817694380946405       1.00      1.00      1.00         7\n",
      "-0.0846918581535552       0.00      0.00      0.00         1\n",
      "-0.0856529291942516       1.00      1.00      1.00         8\n",
      "-0.0931497807868758       0.00      0.00      0.00         2\n",
      " -0.111454939107718       0.00      0.00      0.00         2\n",
      " -0.115068299600731       0.00      0.00      0.00         1\n",
      " -0.119061353127397       0.00      0.00      0.00         1\n",
      " -0.121022702145616       0.00      0.00      0.00         2\n",
      " -0.121983773186312       0.00      0.00      0.00         3\n",
      " -0.124906193245227       0.00      0.00      0.00         1\n",
      " -0.125597133679325       1.00      1.00      1.00         9\n",
      "  -0.12851955373824       1.00      1.00      1.00         3\n",
      " -0.129480624778936       1.00      1.00      1.00         2\n",
      " -0.132403044837851       0.00      0.00      0.00         2\n",
      " -0.135325464896766       0.00      0.00      0.00         1\n",
      "  -0.15042873050907       0.00      0.00      0.00         1\n",
      " -0.158314617178373       0.00      0.00      0.00         4\n",
      " -0.162198108277984       0.00      0.00      0.00         1\n",
      " -0.162889048712082       1.00      1.00      1.00         1\n",
      " -0.163822364870522       0.00      0.00      0.00         2\n",
      " -0.164850397730301       0.00      0.00      0.00         2\n",
      " -0.165811468770997       1.00      1.00      1.00        15\n",
      " -0.168733888829912       1.00      1.00      1.00        34\n",
      " -0.169694959870608       0.00      0.00      0.00         1\n",
      " -0.176230740422536       0.00      0.00      0.00         1\n",
      " -0.179153160481451       0.00      0.00      0.00         2\n",
      " -0.183837154442216       0.00      0.00      0.00         1\n",
      " -0.202142312763057       1.00      1.00      1.00         4\n",
      " -0.205064732821972       1.00      1.00      1.00         2\n",
      " -0.208678093314985       0.00      0.00      0.00         5\n",
      " -0.209639164355682       0.00      0.00      0.00         1\n",
      " -0.212561584414596       0.00      0.00      0.00         1\n",
      " -0.245970008347742       1.00      1.00      1.00         2\n",
      " -0.248892428406657       1.00      1.00      1.00         2\n",
      " -0.250786815605793       0.00      0.00      0.00         1\n",
      " -0.255698339565183       0.00      0.00      0.00         2\n",
      " -0.270801605177487       0.00      0.00      0.00         2\n",
      " -0.284195239538939       0.00      0.00      0.00         1\n",
      " -0.285223272398717       0.00      0.00      0.00         1\n",
      " -0.286184343439414       0.00      0.00      0.00         3\n",
      " -0.289106763498329       1.00      1.00      1.00         8\n",
      "  -0.29299025459794       0.00      0.00      0.00         1\n",
      " -0.329050967983402       0.00      0.00      0.00         1\n",
      " -0.336899757727423       0.00      0.00      0.00         1\n",
      " -0.340513118220436       0.00      0.00      0.00         1\n",
      " -0.362701767640889       0.00      0.00      0.00         1\n",
      " -0.367276199174599       0.00      0.00      0.00         1\n",
      " -0.370198619233513       0.00      0.00      0.00         2\n",
      " -0.372187723133989       0.00      0.00      0.00         1\n",
      " -0.373230601719483       0.00      0.00      0.00         1\n",
      " -0.377805033253193       0.00      0.00      0.00         2\n",
      " -0.379684574726613       0.00      0.00      0.00         1\n",
      " -0.380727453312108       1.00      1.00      1.00         8\n",
      " -0.383759435798078       0.00      0.00      0.00         1\n",
      "  -0.40749053426627       0.00      0.00      0.00         2\n",
      " -0.409479638166746       0.00      0.00      0.00         3\n",
      " -0.417058297304168       1.00      1.00      1.00         4\n",
      " -0.420941788403779       0.00      0.00      0.00         1\n",
      " -0.427587131382763       0.00      0.00      0.00         1\n",
      " -0.431470622482374       0.00      0.00      0.00         1\n",
      " -0.450357158810259       0.00      0.00      0.00         1\n",
      " -0.460885992888853       1.00      1.00      1.00         3\n",
      " -0.464769483988464       0.00      0.00      0.00         1\n",
      " -0.471414826967448       1.00      1.00      1.00        11\n",
      " -0.475298318067059       0.00      0.00      0.00         1\n",
      " -0.478220738125973       0.00      0.00      0.00         1\n",
      "   -0.4936034763879       1.00      1.00      1.00         7\n",
      "  -0.49817790792161       0.00      0.00      0.00         1\n",
      " -0.500139256939828       0.00      0.00      0.00         2\n",
      " -0.501100327980525       1.00      1.00      1.00         3\n",
      " -0.504022748039439       0.00      0.00      0.00         4\n",
      " -0.510668091018423       0.00      0.00      0.00         1\n",
      " -0.522048433710658       0.00      0.00      0.00         1\n",
      " -0.537431171972585       0.00      0.00      0.00         5\n",
      "   -0.5403535920315       1.00      1.00      1.00         1\n",
      " -0.543966952524513       0.00      0.00      0.00         1\n",
      "  -0.54796000605118       0.00      0.00      0.00         3\n",
      " -0.551843497150791       0.00      0.00      0.00         1\n",
      " -0.554495786603107       0.00      0.00      0.00         2\n",
      "  -0.56226276880233       0.00      0.00      0.00         1\n",
      " -0.565185188861244       0.00      0.00      0.00         1\n",
      "  -0.57365245353759       0.00      0.00      0.00         1\n",
      "  -0.57668443602356       0.00      0.00      0.00         3\n",
      "  -0.58125886755727       0.00      0.00      0.00         2\n",
      " -0.584181287616185       0.00      0.00      0.00         3\n",
      " -0.587213270102155       0.00      0.00      0.00         1\n",
      " -0.591787701635864       1.00      1.00      1.00         7\n",
      " -0.594710121694779       1.00      1.00      1.00        14\n",
      "  -0.59859361279439       0.00      0.00      0.00         2\n",
      " -0.605129393346318       0.00      0.00      0.00         1\n",
      " -0.613866788629262       0.00      0.00      0.00         1\n",
      " -0.620512131608245       1.00      1.00      1.00         1\n",
      " -0.621473202648942       0.00      0.00      0.00         4\n",
      " -0.624395622707856       0.00      0.00      0.00         3\n",
      " -0.630350025252741       0.00      0.00      0.00         1\n",
      "  -0.63104096568684       1.00      1.00      1.00         1\n",
      " -0.634924456786451       1.00      1.00      1.00         4\n",
      " -0.660726466699917       0.00      0.00      0.00         2\n",
      " -0.663648886758831       0.00      0.00      0.00         1\n",
      "  -0.66433982719293       1.00      1.00      1.00         4\n",
      " -0.668332880719597       0.00      0.00      0.00         2\n",
      " -0.671255300778511       0.00      0.00      0.00         2\n",
      " -0.674177720837426       0.00      0.00      0.00         2\n",
      " -0.674868661271524       1.00      1.00      1.00         7\n",
      " -0.677791081330439       0.00      0.00      0.00         2\n",
      "  -0.68167457243005       0.00      0.00      0.00         2\n",
      " -0.704554162284602       0.00      0.00      0.00         1\n",
      " -0.707586144770572       0.00      0.00      0.00         1\n",
      " -0.711469635870183       0.00      0.00      0.00         1\n",
      " -0.712160576304281       0.00      0.00      0.00         3\n",
      " -0.713093892462721       0.00      0.00      0.00         1\n",
      " -0.715082996363196       1.00      1.00      1.00         2\n",
      " -0.718005416422111       1.00      1.00      1.00        18\n",
      " -0.744768497376273       0.00      0.00      0.00         1\n",
      " -0.751413840355257       1.00      1.00      1.00         6\n",
      " -0.755297331454868       0.00      0.00      0.00         1\n",
      " -0.761833112006796       0.00      0.00      0.00         1\n",
      " -0.765716603106407       0.00      0.00      0.00         1\n",
      " -0.787635121920262       0.00      0.00      0.00         1\n",
      " -0.790667104406232       1.00      1.00      1.00         5\n",
      " -0.791600420564671       0.00      0.00      0.00         1\n",
      " -0.794550595505843       0.00      0.00      0.00         1\n",
      " -0.795241535939942       1.00      1.00      1.00         3\n",
      " -0.796174852098381       0.00      0.00      0.00         1\n",
      " -0.798163955998856       1.00      1.00      1.00         4\n",
      " -0.828892335597428       0.00      0.00      0.00         3\n",
      " -0.834494799990917       1.00      1.00      1.00         7\n",
      " -0.835455871031613       0.00      0.00      0.00         1\n",
      " -0.838378291090528       1.00      1.00      1.00         8\n",
      " -0.840382240716719       0.00      0.00      0.00         1\n",
      " -0.841300711149442       0.00      0.00      0.00         1\n",
      " -0.848797562742067       0.00      0.00      0.00         1\n",
      " -0.872720031182113       1.00      1.00      1.00         4\n",
      " -0.874709135082588       0.00      0.00      0.00         2\n",
      " -0.875642451241028       0.00      0.00      0.00         1\n",
      " -0.875670206123285       0.00      0.00      0.00         1\n",
      " -0.878322495575601       1.00      1.00      1.00         1\n",
      " -0.879255811734041       0.00      0.00      0.00         1\n",
      " -0.885210214278926       0.00      0.00      0.00         1\n",
      " -0.889784645812635       0.00      0.00      0.00         1\n",
      " -0.911973295233088       1.00      1.00      1.00         6\n",
      " -0.916547726766798       1.00      1.00      1.00         7\n",
      " -0.927076560845392       1.00      1.00      1.00         2\n",
      " -0.929998980904307       0.00      0.00      0.00         2\n",
      " -0.930960051945003       0.00      0.00      0.00         1\n",
      " -0.955800990817773       1.00      1.00      1.00        15\n",
      " -0.956762061858469       0.00      0.00      0.00         3\n",
      " -0.958751165758945       0.00      0.00      0.00         1\n",
      " -0.966329824896367       1.00      1.00      1.00         2\n",
      " -0.996015325909445       0.00      0.00      0.00         1\n",
      " -0.999628686402458       1.00      1.00      1.00        11\n",
      "  -1.00558308894734       0.00      0.00      0.00         1\n",
      "  -1.00654415998804       0.00      0.00      0.00         2\n",
      "  -1.01015752048105       1.00      1.00      1.00         4\n",
      "  -1.03984302149413       1.00      1.00      1.00         1\n",
      "   -1.0428750039801       1.00      1.00      1.00         1\n",
      "  -1.04744943551381       0.00      0.00      0.00         3\n",
      "  -1.04941078453203       0.00      0.00      0.00         1\n",
      "  -1.05037185557272       1.00      1.00      1.00         1\n",
      "  -1.05329427563164       1.00      1.00      1.00         2\n",
      "  -1.06079112722426       0.00      0.00      0.00         1\n",
      "   -1.0800573565858       0.00      0.00      0.00         1\n",
      "  -1.08670269956478       1.00      1.00      1.00         5\n",
      "   -1.0896251196237       0.00      0.00      0.00         4\n",
      "  -1.12292398112979       0.00      0.00      0.00         1\n",
      "  -1.12595596361576       1.00      1.00      1.00        10\n",
      "  -1.12983945471537       0.00      0.00      0.00         1\n",
      "  -1.13053039514947       1.00      1.00      1.00         6\n",
      "   -1.1363752352673       0.00      0.00      0.00         1\n",
      "  -1.16313831622146       0.00      0.00      0.00         1\n",
      "  -1.16978365920044       1.00      1.00      1.00        10\n",
      "  -1.17366715030006       0.00      0.00      0.00         3\n",
      "  -1.21292041435103       0.00      0.00      0.00         1\n",
      "  -1.21361135478513       1.00      1.00      1.00         5\n",
      "  -1.22146014452915       0.00      0.00      0.00         1\n",
      "  -1.24925125834309       0.00      0.00      0.00         1\n",
      "  -1.25021232938379       0.00      0.00      0.00         1\n",
      "   -1.2538256898768       0.00      0.00      0.00         5\n",
      "   -1.3016186841059       0.00      0.00      0.00         2\n",
      "  -1.33329328901945       0.00      0.00      0.00         1\n",
      "  -1.33690664951246       0.00      0.00      0.00         2\n",
      "  -1.33891059913865       0.00      0.00      0.00         2\n",
      "  -1.34475543925648       0.00      0.00      0.00         1\n",
      "  -1.34544637969058       0.00      0.00      0.00         1\n",
      "  -1.37816386318963       1.00      1.00      1.00         2\n",
      "  -1.38469964374156       0.00      0.00      0.00         1\n",
      "  -1.38566071478225       0.00      0.00      0.00         1\n",
      "  -1.38858313484117       0.00      0.00      0.00         1\n",
      "  -1.42199155877431       1.00      1.00      1.00         7\n",
      "  -1.42491397883323       0.00      0.00      0.00         1\n",
      "  -1.46116301528049       0.00      0.00      0.00         1\n",
      "  -1.46124482282529       0.12      1.00      0.21        74\n",
      "  -1.46312436429871       0.00      0.00      0.00         1\n",
      "    -1.465819254359       1.00      1.00      1.00        10\n",
      "  -1.46874167441791       0.00      0.00      0.00         1\n",
      "  -1.50145915791696       0.00      0.00      0.00         1\n",
      "  -1.50507251840997       1.00      1.00      1.00        32\n",
      "  -1.50603358945067       0.00      0.00      0.00         2\n",
      "  -1.50895600950958       0.00      0.00      0.00         1\n",
      "  -1.54528685350164       0.00      0.00      0.00         3\n",
      "  -1.54890021399466       1.00      1.00      1.00        22\n",
      "  -1.55474505411249       0.00      0.00      0.00         3\n",
      "  -1.58454011755262       0.00      0.00      0.00         5\n",
      "  -1.58550118859332       0.00      0.00      0.00         1\n",
      "  -1.58911454908633       1.00      1.00      1.00         9\n",
      "   -1.6283678131373       1.00      1.00      1.00         1\n",
      "    -1.629328884178       0.00      0.00      0.00         3\n",
      "  -1.67219550872199       1.00      1.00      1.00         3\n",
      "  -1.71240984381366       0.00      0.00      0.00         1\n",
      "  -1.79549080344932       1.00      1.00      1.00        17\n",
      " 0.0013115215410194       0.00      0.00      0.00         5\n",
      " 0.0118403556196139       0.00      0.00      0.00         5\n",
      " 0.0272230938815412       0.00      0.00      0.00         2\n",
      " 0.0301455139404558       0.00      0.00      0.00         1\n",
      " 0.0347199454741653       1.00      1.00      1.00         3\n",
      " 0.0376423655330801       0.00      0.00      0.00         3\n",
      " 0.0452487795527597       1.00      1.00      1.00         1\n",
      " 0.0481711996116745       1.00      1.00      1.00         3\n",
      " 0.0710507894662259       0.00      0.00      0.00         2\n",
      " 0.0720118605069223       0.00      0.00      0.00         1\n",
      "  0.074934280565837       0.00      0.00      0.00         1\n",
      " 0.0778567006247517       1.00      1.00      1.00        15\n",
      " 0.0785476410588501       0.00      0.00      0.00         1\n",
      " 0.0815796235448203       0.00      0.00      0.00         1\n",
      " 0.0825129397032597       0.00      0.00      0.00         1\n",
      " 0.0890764751374445       0.00      0.00      0.00         1\n",
      " 0.0919988951963592       0.00      0.00      0.00         1\n",
      " 0.0949213152552739       0.00      0.00      0.00         1\n",
      "  0.110318899242917       0.00      0.00      0.00         1\n",
      "   0.11884378369532       0.00      0.00      0.00         1\n",
      "  0.121684396209436       0.00      0.00      0.00         2\n",
      "  0.132213230288031       0.00      0.00      0.00         2\n",
      "  0.138858573267014       0.00      0.00      0.00         1\n",
      "  0.150518388608873       0.00      0.00      0.00         1\n",
      "  0.158015240201497       0.00      0.00      0.00         1\n",
      "  0.159058118786992       0.00      0.00      0.00         1\n",
      "  0.168544074280091       1.00      1.00      1.00         6\n",
      "  0.169586952865586       0.00      0.00      0.00         1\n",
      "  0.171466494339006       0.00      0.00      0.00         3\n",
      "  0.175079854832019       0.00      0.00      0.00         1\n",
      "  0.194346084193558       0.00      0.00      0.00         2\n",
      "  0.202885814371677       0.00      0.00      0.00         1\n",
      "  0.215294189923691       0.00      0.00      0.00         2\n",
      "  0.220220559608797       0.00      0.00      0.00         1\n",
      "  0.239216658363737       1.00      1.00      1.00         2\n",
      "  0.249635930015276       0.00      0.00      0.00         1\n",
      "  0.260164764093871       1.00      1.00      1.00         3\n",
      "  0.262126113112089       0.00      0.00      0.00         1\n",
      "  0.263087184152785       0.00      0.00      0.00         1\n",
      "  0.283044353948422       0.00      0.00      0.00         1\n",
      "  0.291839369007423       0.00      0.00      0.00         4\n",
      "  0.296495608085931       0.00      0.00      0.00         1\n",
      "  0.297456679126627       0.00      0.00      0.00         1\n",
      "  0.300379099185542       0.00      0.00      0.00         2\n",
      "  0.303992459678555       0.00      0.00      0.00         3\n",
      "  0.322297617999397       0.00      0.00      0.00         1\n",
      "  0.334815555978467       0.00      0.00      0.00         1\n",
      "  0.335667064592108       0.00      0.00      0.00         1\n",
      "  0.335748872136906       0.00      0.00      0.00         1\n",
      "  0.336709943177603       0.00      0.00      0.00         1\n",
      "  0.339632363236517       0.00      0.00      0.00         1\n",
      "  0.340593434277214       0.00      0.00      0.00         1\n",
      "  0.344206794770227       0.00      0.00      0.00         1\n",
      "  0.373040787169663       0.00      0.00      0.00         1\n",
      "  0.375963207228578       0.00      0.00      0.00         2\n",
      "  0.380537638762288       1.00      1.00      1.00        74\n",
      "  0.383460058821202       1.00      1.00      1.00         6\n",
      "  0.390956910413826       0.00      0.00      0.00         1\n",
      "  0.406339648675754       0.00      0.00      0.00         1\n",
      "  0.409262068734668       0.00      0.00      0.00         1\n",
      "  0.416868482754348       0.00      0.00      0.00         3\n",
      "  0.419790902813263       1.00      1.00      1.00         4\n",
      "  0.420751973853959       0.00      0.00      0.00         1\n",
      "  0.423674393912874       1.00      1.00      1.00         2\n",
      "  0.424365334346972       0.00      0.00      0.00         1\n",
      "  0.427287754405887       0.00      0.00      0.00         2\n",
      "  0.430210174464802       0.00      0.00      0.00         1\n",
      "  0.453089764319353       0.00      0.00      0.00         1\n",
      "  0.453199326746409       0.00      0.00      0.00         2\n",
      "  0.456121746805323       1.00      1.00      1.00         1\n",
      "   0.45708281784602       0.00      0.00      0.00         2\n",
      "  0.459044166864238       0.00      0.00      0.00         1\n",
      "  0.460005237904934       0.00      0.00      0.00         1\n",
      "  0.463618598397948       1.00      1.00      1.00        19\n",
      "  0.467502089497559       0.00      0.00      0.00         2\n",
      "  0.474037870049486       0.00      0.00      0.00         1\n",
      "  0.485807247818401       0.00      0.00      0.00         1\n",
      "   0.49341366183808       0.00      0.00      0.00         1\n",
      "  0.496336081896995       0.00      0.00      0.00         1\n",
      "  0.499949442390008       1.00      1.00      1.00         1\n",
      "  0.500910513430704       1.00      1.00      1.00         7\n",
      "  0.501953392016199       0.00      0.00      0.00         1\n",
      "  0.502871862448923       1.00      1.00      1.00         2\n",
      "  0.503832933489619       1.00      1.00      1.00        47\n",
      "  0.504794004530316       0.00      0.00      0.00         1\n",
      "  0.506755353548534       1.00      1.00      1.00         8\n",
      "  0.507446293982632       0.00      0.00      0.00         1\n",
      "  0.529634943403085       0.00      0.00      0.00         1\n",
      "  0.536280286382069       0.00      0.00      0.00         1\n",
      "  0.537241357422765       0.00      0.00      0.00         2\n",
      "   0.54016377748168       1.00      1.00      1.00         5\n",
      "  0.540854717915778       0.00      0.00      0.00         1\n",
      "  0.544047268581291       1.00      1.00      1.00         4\n",
      "  0.546699558033608       1.00      1.00      1.00         4\n",
      "  0.547660629074304       1.00      1.00      1.00         6\n",
      "  0.550583049133219       1.00      1.00      1.00         5\n",
      "   0.57649462147374       1.00      1.00      1.00         1\n",
      "  0.579417041532655       0.00      0.00      0.00         1\n",
      "  0.583991473066364       1.00      1.00      1.00         8\n",
      "  0.586913893125279       1.00      1.00      1.00         1\n",
      "  0.587874964165976       0.00      0.00      0.00         3\n",
      "  0.590527253618292       0.00      0.00      0.00         1\n",
      "  0.609903045406886       0.00      0.00      0.00         1\n",
      "  0.620322317058425       0.00      0.00      0.00         3\n",
      "   0.62136519564392       0.00      0.00      0.00         1\n",
      "   0.62324473711734       1.00      1.00      1.00         6\n",
      "  0.624205808158036       0.00      0.00      0.00         3\n",
      "  0.624287615702835       0.00      0.00      0.00         1\n",
      "  0.627128228216951       1.00      1.00      1.00        30\n",
      "  0.630741588709964       0.00      0.00      0.00         1\n",
      "  0.660536652150097       0.00      0.00      0.00         1\n",
      "  0.663459072209011       0.00      0.00      0.00         1\n",
      "  0.667072432702024       1.00      1.00      1.00         4\n",
      "  0.669076382328216       0.00      0.00      0.00         1\n",
      "  0.670955923801635       1.00      1.00      1.00         7\n",
      "  0.671037731346434       0.00      0.00      0.00         1\n",
      "  0.699789916201072       0.00      0.00      0.00         1\n",
      "  0.704364347734782       0.00      0.00      0.00         2\n",
      "  0.707286767793696       0.00      0.00      0.00         2\n",
      "  0.710209187852611       0.00      0.00      0.00         1\n",
      "  0.715826497971815       0.00      0.00      0.00         1\n",
      "  0.743617611785757       1.00      1.00      1.00         2\n",
      "  0.751114463378381       0.00      0.00      0.00         1\n",
      "  0.752157341963876       1.00      1.00      1.00         1\n",
      "  0.787445307370441       0.00      0.00      0.00         1\n",
      "  0.788488185955936       0.00      0.00      0.00         4\n",
      "  0.790367727429356       0.00      0.00      0.00         1\n",
      "  0.790477289856412       0.00      0.00      0.00         1\n",
      "  0.791410606014851       0.00      0.00      0.00         1\n",
      "  0.792371677055547       0.00      0.00      0.00         1\n",
      "  0.798907457607475       1.00      1.00      1.00         1\n",
      "  0.828702521047608       0.00      0.00      0.00         1\n",
      "  0.835238301599536       0.00      0.00      0.00         2\n",
      "  0.838160721658451       0.00      0.00      0.00         2\n",
      "  0.875452636691207       1.00      1.00      1.00         4\n",
      "  0.881988417243135       0.00      0.00      0.00         1\n",
      "  0.885981470769802       0.00      0.00      0.00         2\n",
      "  0.911783480683268       1.00      1.00      1.00         5\n",
      "  0.919280332275892       0.00      0.00      0.00         2\n",
      "   0.92884809531379       0.00      0.00      0.00         1\n",
      "  0.929809166354487       0.00      0.00      0.00         3\n",
      "  0.936344946906414       0.00      0.00      0.00         1\n",
      "  0.948114324675329       0.00      0.00      0.00         1\n",
      "  0.955611176267953       1.00      1.00      1.00         3\n",
      "  0.956654054853448       0.00      0.00      0.00         1\n",
      "  0.957600280168428       0.00      0.00      0.00         1\n",
      "  0.958533596326868       0.00      0.00      0.00         3\n",
      "  0.966140010346547       0.00      0.00      0.00         1\n",
      "  0.969062430405462       0.00      0.00      0.00         1\n",
      "  0.994864440318928       0.00      0.00      0.00         1\n",
      "  0.998747931418539       0.00      0.00      0.00         1\n",
      "   1.00236129191155       0.00      0.00      0.00         1\n",
      "   1.01289012599015       1.00      1.00      1.00         3\n",
      "   1.04257562700322       0.00      0.00      0.00         1\n",
      "   1.04629854992329       0.00      0.00      0.00         1\n",
      "   1.05310446108182       0.00      0.00      0.00         2\n",
      "   1.09597108562581       0.00      0.00      0.00         1\n",
      "   1.12377704516546       0.00      0.00      0.00         1\n",
      "   1.12576614906594       0.00      0.00      0.00         2\n",
      "   1.12669946522438       0.00      0.00      0.00         1\n",
      "   1.13326300065856       1.00      1.00      1.00         3\n",
      "   1.13618542071748       0.00      0.00      0.00         1\n",
      "   1.17052716080906       0.00      0.00      0.00         1\n",
      "   1.20696756722818       0.00      0.00      0.00         1\n",
      "    1.2098081797423       0.00      0.00      0.00         1\n",
      "   1.21634396029422       0.00      0.00      0.00         3\n",
      "   1.22030925893863       0.00      0.00      0.00         1\n",
      "    1.2461112688521       0.00      0.00      0.00         1\n",
      "   1.25760117397139       0.00      0.00      0.00         1\n",
      "    1.2612145344644       0.00      0.00      0.00         1\n",
      "   1.26509802556401       0.00      0.00      0.00         1\n",
      "   1.26520758799107       0.00      0.00      0.00         1\n",
      "   1.28632560394377       0.00      0.00      0.00         1\n",
      "   1.29288913937796       0.00      0.00      0.00         2\n",
      "   1.29850644949716       0.00      0.00      0.00         1\n",
      "   1.30142886955607       1.00      1.00      1.00         4\n",
      "   1.30435128961499       0.00      0.00      0.00         1\n",
      "   1.33775971354814       0.00      0.00      0.00         3\n",
      "   1.34068213360705       0.00      0.00      0.00         3\n",
      "   1.34817898519967       0.00      0.00      0.00         3\n",
      "   1.37701297759911       0.00      0.00      0.00         1\n",
      "   1.37797404863981       0.00      0.00      0.00         1\n",
      "   1.38089646869872       0.00      0.00      0.00         1\n",
      "   1.38158740913282       0.00      0.00      0.00         2\n",
      "   1.38450982919173       1.00      1.00      1.00         4\n",
      "   1.38743224925065       0.00      0.00      0.00         2\n",
      "   1.41419533020481       0.00      0.00      0.00         1\n",
      "    1.4208406731838       0.00      0.00      0.00         1\n",
      "   1.42284462280999       0.00      0.00      0.00         1\n",
      "   1.42376309324271       0.00      0.00      0.00         1\n",
      "   1.42472416428341       1.00      1.00      1.00         3\n",
      "   1.42833752477642       0.00      0.00      0.00         1\n",
      "   1.45344859425579       0.00      0.00      0.00         1\n",
      "   1.45813258821655       0.00      0.00      0.00         1\n",
      "   1.46105500827547       1.00      1.00      1.00         1\n",
      "   1.46201607931616       0.00      0.00      0.00         1\n",
      "   1.46397742833438       0.00      0.00      0.00         1\n",
      "   1.46855185986809       1.00      1.00      1.00         5\n",
      "   1.49738585226753       0.00      0.00      0.00         1\n",
      "   1.50488270386015       1.00      1.00      1.00         7\n",
      "   1.50780512391907       1.00      1.00      1.00         1\n",
      "   1.50876619495976       0.00      0.00      0.00         1\n",
      "   1.51833395799766       0.00      0.00      0.00         1\n",
      "   1.59184715459542       0.00      0.00      0.00         1\n",
      "   1.62817799858748       0.00      0.00      0.00         1\n",
      "   1.62922087717298       0.00      0.00      0.00         1\n",
      "   1.64524261321801       0.00      0.00      0.00         1\n",
      "   1.67200569417217       0.00      0.00      0.00         2\n",
      "   1.67304857275766       1.00      1.00      1.00         3\n",
      "   1.71230183680864       0.00      0.00      0.00         1\n",
      "    1.7486326808007       0.00      0.00      0.00         1\n",
      "   1.75612953239332       0.00      0.00      0.00         1\n",
      "   1.76001302349293       0.00      0.00      0.00         1\n",
      "   1.76561548788642       0.00      0.00      0.00         2\n",
      "    1.7953827964443       0.00      0.00      0.00         1\n",
      "   1.79634386748499       1.00      1.00      1.00         3\n",
      "   1.83267471147706       0.00      0.00      0.00         1\n",
      "   1.83363578251775       0.00      0.00      0.00         1\n",
      "   1.83559713153597       0.00      0.00      0.00         1\n",
      "   1.88703124114033       0.00      0.00      0.00         1\n",
      "   1.89745051279187       0.00      0.00      0.00         1\n",
      "    1.9137943220945       0.00      0.00      0.00         1\n",
      "   1.91963916221233       0.00      0.00      0.00         1\n",
      "    1.9233620851324       0.00      0.00      0.00         1\n",
      "   1.92421359374604       0.00      0.00      0.00         1\n",
      "   1.92724557623201       0.00      0.00      0.00         1\n",
      "   1.93085893672502       0.00      0.00      0.00         1\n",
      "   1.95597000620439       0.00      0.00      0.00         1\n",
      "   1.99979770178907       0.00      0.00      0.00         1\n",
      "   2.03612854578113       0.00      0.00      0.00         1\n",
      "   2.04466827595925       0.00      0.00      0.00         1\n",
      "   2.05707665151127       0.00      0.00      0.00         1\n",
      "   2.05908060113746       0.00      0.00      0.00         1\n",
      "   2.10090434709595       0.00      0.00      0.00         1\n",
      "   2.13270336016227       0.00      0.00      0.00         1\n",
      "   2.13734475351507       0.00      0.00      0.00         1\n",
      "    2.1392391407142       0.00      0.00      0.00         1\n",
      "   2.16796357068658       0.00      0.00      0.00         1\n",
      "    2.1859892563578       0.00      0.00      0.00         1\n",
      "   2.20817790577825       0.00      0.00      0.00         1\n",
      "   2.21870673985685       0.00      0.00      0.00         1\n",
      "   2.25865094434192       0.00      0.00      0.00         1\n",
      "   2.25958426050036       0.00      0.00      0.00         1\n",
      "   2.26157336440084       0.00      0.00      0.00         2\n",
      "   2.26253443544153       0.00      0.00      0.00         2\n",
      "   2.26907021599346       0.00      0.00      0.00         1\n",
      "   2.30540105998552       0.00      0.00      0.00         1\n",
      "   2.34269297501828       0.00      0.00      0.00         1\n",
      "   2.34561539507719       1.00      1.00      1.00         5\n",
      "   2.34761934470339       0.00      0.00      0.00         1\n",
      "   2.38290731010995       0.00      0.00      0.00         1\n",
      "   2.38944309066188       0.00      0.00      0.00         1\n",
      "   2.42119950312023       0.00      0.00      0.00         1\n",
      "   2.42577393465394       0.00      0.00      0.00         1\n",
      "   2.42869635471285       0.00      0.00      0.00         1\n",
      "   2.46598826974561       0.00      0.00      0.00         1\n",
      "   2.46891068980453       0.00      0.00      0.00         1\n",
      "   2.51273838538921       0.00      0.00      0.00         1\n",
      "   2.55011210796677       0.00      0.00      0.00         1\n",
      "   2.58540007337333       0.00      0.00      0.00         1\n",
      "   2.63415413864312       0.00      0.00      0.00         1\n",
      "   2.75648836232976       0.00      0.00      0.00         1\n",
      "   2.89289781876892       0.00      0.00      0.00         1\n",
      "   2.93121776666145       0.00      0.00      0.00         1\n",
      "   3.14321133114365       0.00      0.00      0.00         1\n",
      "   3.14613375120256       0.00      0.00      0.00         1\n",
      "   3.18634808629424       0.00      0.00      0.00         1\n",
      "   3.35708443709927       0.00      0.00      0.00         1\n",
      "   3.42613276459037       0.00      0.00      0.00         1\n",
      "   3.44216934636112       0.00      0.00      0.00         1\n",
      "   3.52067587446307       0.00      0.00      0.00         1\n",
      "   3.52924335952344       0.00      0.00      0.00         1\n",
      "   3.60844082805949       0.00      0.00      0.00         1\n",
      "    3.7614216237999       0.00      0.00      0.00         1\n",
      "   4.06410256193744       0.00      0.00      0.00         1\n",
      "\n",
      "           accuracy                           0.62      1433\n",
      "          macro avg       0.23      0.23      0.23      1433\n",
      "       weighted avg       0.57      0.62      0.58      1433\n",
      "\n",
      "Submission file saved: submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nh013\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nh013\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nh013\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PERFORM NAIVE BAYES MODEL\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#DATASET\n",
    "summaries_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_train.csv\"\n",
    "summaries_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_test.csv\"\n",
    "prompts_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_train.csv\"\n",
    "prompts_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_test.csv\"\n",
    "\n",
    "summaries_train_df = pd.read_csv(summaries_train_path)\n",
    "summaries_test_df = pd.read_csv(summaries_test_path)\n",
    "prompts_train_df = pd.read_csv(prompts_train_path)\n",
    "prompts_test_df = pd.read_csv(prompts_test_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'content' and 'wording' COLUMNS ARE CONVERTED TO STRING\n",
    "summaries_train_df['content'] = summaries_train_df['content'].astype(str)\n",
    "summaries_train_df['wording'] = summaries_train_df['wording'].astype(str)\n",
    "\n",
    "#COMBINE TEXT DATA FROM SUMMARAISE AND PROMPTS DATAFRANE\n",
    "summaries_train_df['text'] = summaries_train_df['content'] + \" \" + summaries_train_df['wording']\n",
    "prompts_train_df['prompt_text'] = prompts_train_df['prompt_question'] + \" \" + prompts_train_df['prompt_title'] + \" \" + prompts_train_df['prompt_text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    \n",
    "    # CONVERT TO LOWERCASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # PREMOVE SPECIAL CHERECTER AND PUNCTUATION\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOPWORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FOR  summaries_train_df\n",
    "summaries_train_df['preprocessed_text'] = summaries_train_df['text'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_content'] = summaries_train_df['content'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_wording'] = summaries_train_df['wording'].apply(preprocess_text)\n",
    "\n",
    "#PREPROCESSING FOR  prompts_train_df\n",
    "prompts_train_df['preprocessed_prompt_text'] = prompts_train_df['prompt_text'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_question'] = prompts_train_df['prompt_question'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_title'] = prompts_train_df['prompt_title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# TOKENIZATION, STEMMING, AND LEMMATIZATION\n",
    "summaries_train_df['tokens'] = summaries_train_df['preprocessed_text'].apply(word_tokenize)\n",
    "summaries_train_df['stemmed_tokens'] = summaries_train_df['tokens'].apply(apply_stemming)\n",
    "summaries_train_df['lemmatized_tokens'] = summaries_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "prompts_train_df['tokens'] = prompts_train_df['preprocessed_prompt_text'].apply(word_tokenize)\n",
    "prompts_train_df['stemmed_tokens'] = prompts_train_df['tokens'].apply(apply_stemming)\n",
    "prompts_train_df['lemmatized_tokens'] = prompts_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "\n",
    "\n",
    "# PART OF SPEECH TAGGING\n",
    "def apply_pos_tagging(tokens):\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "summaries_train_df['pos_tags'] = summaries_train_df['tokens'].apply(apply_pos_tagging)\n",
    "prompts_train_df['pos_tags'] = prompts_train_df['tokens'].apply(apply_pos_tagging)\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "def apply_tfidf(tokens):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(tokens)])\n",
    "    return tfidf_matrix\n",
    "\n",
    "summaries_train_df['tfidf_matrix'] = summaries_train_df['lemmatized_tokens'].apply(apply_tfidf)\n",
    "prompts_train_df['tfidf_matrix'] = prompts_train_df['lemmatized_tokens'].apply(apply_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SPLIT DATA\n",
    "X = summaries_train_df['content'] + \" \" + summaries_train_df['preprocessed_content'] + \" \" + summaries_train_df['preprocessed_wording']\n",
    "y = summaries_train_df['wording']  # You need to create the 'label' column with class labels\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAINING AND TESTING SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "#LET'S CREATE TF-IDF VECTORE\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# TRAIN MODEL\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# PREDICTIONS\n",
    "y_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# EVALUATION\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "#PREPROESS THE TEST DATA\n",
    "summaries_test_df['preprocessed_text'] = summaries_test_df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# CREATE TF-IDF VECTORS FOR TEST DATA \n",
    "X_test_submission = summaries_test_df['preprocessed_text']\n",
    "X_test_submission_tfidf = tfidf_vectorizer.transform(X_test_submission)\n",
    "\n",
    "# PREDICTION ON TEST DATA \n",
    "y_test_submission_pred = nb_model.predict(X_test_submission_tfidf)\n",
    "\n",
    "# SUBMISSSION DATA FRAME \n",
    "submission_df = pd.DataFrame({\n",
    "    'student_id': summaries_test_df['student_id'],\n",
    "    'content': summaries_test_df['text'],\n",
    "    'wording': y_test_submission_pred\n",
    "})\n",
    "\n",
    "# SAVE SUBMISSION FILE\n",
    "submission_file_path = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(\"Submission file saved:\", submission_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc7aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a675935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5416de42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "180/180 [==============================] - 15s 31ms/step - loss: 0.3157 - val_loss: 0.0695\n",
      "Epoch 2/100\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.0362 - val_loss: 0.0546\n",
      "Epoch 3/100\n",
      "180/180 [==============================] - 4s 21ms/step - loss: 0.0168 - val_loss: 0.0470\n",
      "Epoch 4/100\n",
      "180/180 [==============================] - 4s 22ms/step - loss: 0.0104 - val_loss: 0.0434\n",
      "Epoch 5/100\n",
      "180/180 [==============================] - 4s 22ms/step - loss: 0.0071 - val_loss: 0.0429\n",
      "Epoch 6/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 0.0046 - val_loss: 0.0409\n",
      "Epoch 7/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0037 - val_loss: 0.0412\n",
      "Epoch 8/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 0.0046 - val_loss: 0.0404\n",
      "Epoch 9/100\n",
      "180/180 [==============================] - 4s 23ms/step - loss: 0.0040 - val_loss: 0.0395\n",
      "Epoch 10/100\n",
      "180/180 [==============================] - 4s 19ms/step - loss: 0.0033 - val_loss: 0.0396\n",
      "Epoch 11/100\n",
      "180/180 [==============================] - 3s 19ms/step - loss: 0.0031 - val_loss: 0.0402\n",
      "Epoch 12/100\n",
      "180/180 [==============================] - 4s 20ms/step - loss: 0.0037 - val_loss: 0.0399\n",
      "Epoch 13/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0042 - val_loss: 0.0399\n",
      "Epoch 14/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 0.0032 - val_loss: 0.0391\n",
      "Epoch 15/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0025 - val_loss: 0.0393\n",
      "Epoch 16/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 0.0022 - val_loss: 0.0397\n",
      "Epoch 17/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0027 - val_loss: 0.0387\n",
      "Epoch 18/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0027 - val_loss: 0.0392\n",
      "Epoch 19/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 0.0028 - val_loss: 0.0399\n",
      "Epoch 20/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0036 - val_loss: 0.0391\n",
      "Epoch 21/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0033 - val_loss: 0.0394\n",
      "Epoch 22/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0023 - val_loss: 0.0376\n",
      "Epoch 23/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0016 - val_loss: 0.0376\n",
      "Epoch 24/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0015 - val_loss: 0.0372\n",
      "Epoch 25/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0015 - val_loss: 0.0376\n",
      "Epoch 26/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0023 - val_loss: 0.0379\n",
      "Epoch 27/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0020 - val_loss: 0.0380\n",
      "Epoch 28/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0030 - val_loss: 0.0387\n",
      "Epoch 29/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 0.0028 - val_loss: 0.0370\n",
      "Epoch 30/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0014 - val_loss: 0.0369\n",
      "Epoch 31/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0010 - val_loss: 0.0378\n",
      "Epoch 32/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0015 - val_loss: 0.0368\n",
      "Epoch 33/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 0.0013 - val_loss: 0.0367\n",
      "Epoch 34/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0014 - val_loss: 0.0370\n",
      "Epoch 35/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0014 - val_loss: 0.0375\n",
      "Epoch 36/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0015 - val_loss: 0.0376\n",
      "Epoch 37/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 0.0019 - val_loss: 0.0370\n",
      "Epoch 38/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0015 - val_loss: 0.0367\n",
      "Epoch 39/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0013 - val_loss: 0.0362\n",
      "Epoch 40/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 7.9388e-04 - val_loss: 0.0362\n",
      "Epoch 41/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 8.6689e-04 - val_loss: 0.0373\n",
      "Epoch 42/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 0.0012 - val_loss: 0.0363\n",
      "Epoch 43/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 9.1111e-04 - val_loss: 0.0361\n",
      "Epoch 44/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 8.8690e-04 - val_loss: 0.0363\n",
      "Epoch 45/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 9.1515e-04 - val_loss: 0.0362\n",
      "Epoch 46/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 0.0010 - val_loss: 0.0364\n",
      "Epoch 47/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 0.0010 - val_loss: 0.0365\n",
      "Epoch 48/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 9.7737e-04 - val_loss: 0.0365\n",
      "Epoch 49/100\n",
      "180/180 [==============================] - 5s 27ms/step - loss: 7.3526e-04 - val_loss: 0.0361\n",
      "Epoch 50/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 5.9003e-04 - val_loss: 0.0359\n",
      "Epoch 51/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 7.2195e-04 - val_loss: 0.0363\n",
      "Epoch 52/100\n",
      "180/180 [==============================] - 5s 27ms/step - loss: 9.2504e-04 - val_loss: 0.0364\n",
      "Epoch 53/100\n",
      "180/180 [==============================] - 4s 20ms/step - loss: 6.5638e-04 - val_loss: 0.0362\n",
      "Epoch 54/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 7.0413e-04 - val_loss: 0.0360\n",
      "Epoch 55/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 6.6216e-04 - val_loss: 0.0361\n",
      "Epoch 56/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 5.2933e-04 - val_loss: 0.0362\n",
      "Epoch 57/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 4.5125e-04 - val_loss: 0.0361\n",
      "Epoch 58/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 4.5492e-04 - val_loss: 0.0357\n",
      "Epoch 59/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 4.7342e-04 - val_loss: 0.0359\n",
      "Epoch 60/100\n",
      "180/180 [==============================] - 5s 28ms/step - loss: 7.3674e-04 - val_loss: 0.0362\n",
      "Epoch 61/100\n",
      "180/180 [==============================] - 5s 27ms/step - loss: 8.8040e-04 - val_loss: 0.0361\n",
      "Epoch 62/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 6.0198e-04 - val_loss: 0.0361\n",
      "Epoch 63/100\n",
      "180/180 [==============================] - 4s 22ms/step - loss: 5.0191e-04 - val_loss: 0.0360\n",
      "Epoch 64/100\n",
      "180/180 [==============================] - 4s 20ms/step - loss: 5.0425e-04 - val_loss: 0.0361\n",
      "Epoch 65/100\n",
      "180/180 [==============================] - 3s 19ms/step - loss: 5.1517e-04 - val_loss: 0.0364\n",
      "Epoch 66/100\n",
      "180/180 [==============================] - 4s 23ms/step - loss: 7.9844e-04 - val_loss: 0.0360\n",
      "Epoch 67/100\n",
      "180/180 [==============================] - 5s 27ms/step - loss: 5.4555e-04 - val_loss: 0.0359\n",
      "Epoch 68/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 3.8835e-04 - val_loss: 0.0360\n",
      "Epoch 69/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 5.2313e-04 - val_loss: 0.0359\n",
      "Epoch 70/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 5.9164e-04 - val_loss: 0.0363\n",
      "Epoch 71/100\n",
      "180/180 [==============================] - 5s 28ms/step - loss: 4.5103e-04 - val_loss: 0.0361\n",
      "Epoch 72/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 3.0344e-04 - val_loss: 0.0358\n",
      "Epoch 73/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 3.3126e-04 - val_loss: 0.0359\n",
      "Epoch 74/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 2.2875e-04 - val_loss: 0.0359\n",
      "Epoch 75/100\n",
      "180/180 [==============================] - 5s 25ms/step - loss: 2.3049e-04 - val_loss: 0.0360\n",
      "Epoch 76/100\n",
      "180/180 [==============================] - 4s 25ms/step - loss: 2.4761e-04 - val_loss: 0.0360\n",
      "Epoch 77/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 3.1075e-04 - val_loss: 0.0362\n",
      "Epoch 78/100\n",
      "180/180 [==============================] - 5s 26ms/step - loss: 2.9063e-04 - val_loss: 0.0361\n",
      "Epoch 79/100\n",
      "180/180 [==============================] - 3s 19ms/step - loss: 4.0800e-04 - val_loss: 0.0363\n",
      "Epoch 80/100\n",
      "180/180 [==============================] - 4s 19ms/step - loss: 4.3610e-04 - val_loss: 0.0364\n",
      "Epoch 81/100\n",
      "180/180 [==============================] - 4s 24ms/step - loss: 4.0186e-04 - val_loss: 0.0363\n",
      "Epoch 82/100\n",
      "180/180 [==============================] - 4s 20ms/step - loss: 6.2970e-04 - val_loss: 0.0373\n",
      "Epoch 83/100\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 4.9075e-04 - val_loss: 0.0364\n",
      "Epoch 84/100\n",
      "180/180 [==============================] - 3s 16ms/step - loss: 3.2449e-04 - val_loss: 0.0364\n",
      "Epoch 85/100\n",
      "180/180 [==============================] - 3s 19ms/step - loss: 2.4071e-04 - val_loss: 0.0363\n",
      "Epoch 86/100\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 1.9597e-04 - val_loss: 0.0364\n",
      "Epoch 87/100\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 2.2243e-04 - val_loss: 0.0363\n",
      "Epoch 88/100\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 2.8431e-04 - val_loss: 0.0365\n",
      "Epoch 89/100\n",
      "180/180 [==============================] - 3s 15ms/step - loss: 2.4887e-04 - val_loss: 0.0367\n",
      "Epoch 90/100\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 2.2688e-04 - val_loss: 0.0365\n",
      "Epoch 91/100\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 1.7241e-04 - val_loss: 0.0365\n",
      "Epoch 92/100\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 1.9121e-04 - val_loss: 0.0367\n",
      "Epoch 93/100\n",
      "180/180 [==============================] - 3s 17ms/step - loss: 2.6144e-04 - val_loss: 0.0371\n",
      "Epoch 94/100\n",
      "180/180 [==============================] - 3s 16ms/step - loss: 3.9652e-04 - val_loss: 0.0367\n",
      "Epoch 95/100\n",
      "180/180 [==============================] - 3s 18ms/step - loss: 2.8747e-04 - val_loss: 0.0369\n",
      "Epoch 96/100\n",
      "180/180 [==============================] - 3s 16ms/step - loss: 2.4933e-04 - val_loss: 0.0368\n",
      "Epoch 97/100\n",
      "180/180 [==============================] - 3s 16ms/step - loss: 2.4155e-04 - val_loss: 0.0370\n",
      "Epoch 98/100\n",
      "180/180 [==============================] - 3s 16ms/step - loss: 3.2247e-04 - val_loss: 0.0376\n",
      "Epoch 99/100\n",
      "180/180 [==============================] - 3s 15ms/step - loss: 3.2978e-04 - val_loss: 0.0377\n",
      "Epoch 100/100\n",
      "180/180 [==============================] - 3s 15ms/step - loss: 3.0697e-04 - val_loss: 0.0373\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0373\n",
      "Validation Loss: 0.03731726482510567\n",
      "45/45 [==============================] - 1s 4ms/step\n",
      "Enter a summary (type 'exit' to quit): \"Summarize at least 3 elements of an ideal tragedy, as described by Aristotle.\",On Tragedy,\"Chapter 13  As the sequel to what has already been said, we must proceed to consider what the poet should aim at, and what he should avoid, in constructing his plots; and by what means the specific effect of Tragedy will be produced.  A perfect tragedy should, as we have seen, be arranged not on the simple but on the complex plan. It should, moreover, imitate actions which excite pity and fear, this being the distinctive mark of tragic imitation. It follows plainly, in the first place, that the change of fortune presented must not be the spectacle of a virtuous man brought from prosperity to adversity: for this moves neither pity nor fear; it merely shocks us. Nor, again, that of a bad man passing from adversity to prosperity: for nothing can be more alien to the spirit of Tragedy; it possesses no single tragic quality; it neither satisfies the moral sense nor calls forth pity or fear. Nor, again, should the downfall of the utter villain be exhibited. A plot of this kind would, doubtless, satisfy the moral sense, but it would inspire neither pity nor fear; for pity is aroused by unmerited misfortune, fear by the misfortune of a man like ourselves. Such an event, therefore, will be neither pitiful nor terrible. There remains, then, the character between these two extremes  that of a man who is not eminently good and just, yet whose misfortune is brought about not by vice or depravity, but by some error of judgement or frailty. He must be one who is highly renowned and prosperous  a personage like Oedipus, Thyestes, or other illustrious men of such families.  A well-constructed plot should, therefore, be single in its issue, rather than double as some maintain. The change of fortune should be not from bad to good, but, reversely, from good to bad. It should come about as the result not of vice, but of some great error or frailty, in a character either such as we have described, or better rather than worse. The practice of the stage bears out our view. At first the poets recounted any legend that came in their way. Now, the best tragedies are founded on the story of a few houses  on the fortunes of Alcmaeon, Oedipus, Orestes, Meleager, Thyestes, Telephus, and those others who have done or suffered something terrible. A tragedy, then, to be perfect according to the rules of art, should be of this construction. Hence they are in error who censure Euripides just because he follows this principle in his plays, many of which end unhappily. It is, as we have said, the right ending. The best proof is that on the stage and in dramatic competition, such plays, if well worked out, are the most tragic in effect; and Euripides, faulty though he may be in the general management of his subject, yet is felt to be the most tragic of the poets.  In the second rank comes the kind of tragedy which some place first. Like the Odyssey, it has a double thread of plot, and also an opposite catastrophe for the good and for the bad. It is accounted the best because of the weakness of the spectators; for the poet is guided in what he writes by the wishes of his audience. The pleasure, however, thence derived is not the true tragic pleasure. It is proper rather to Comedy, where those who, in the piece, are the deadliest enemies  like Orestes and Aegisthus  quit the stage as friends at the close, and no one slays or is slain.\"\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted Summary Quality: 1.7754551\n",
      "Enter a summary (type 'exit' to quit): \"In complete sentences, summarize the structure of the ancient Egyptian system of government. How were different social classes involved in this government? Cite evidence from the text.\",Egyptian Social Structure,\"Egyptian society was structured like a pyramid. At the top were the gods, such as Ra, Osiris, and Isis. Egyptians believed that the gods controlled the universe. Therefore, it was important to keep them happy. They could make the Nile overflow, cause famine, or even bring death.  The Egyptians also elevated some human beings to gods. Their leaders, called pharaohs, were believed to be gods in human form. They had absolute power over their subjects. After pharaohs died, huge stone pyramids were built as their tombs. Pharaohs were buried in chambers within the pyramids.  Because the people of Egypt believed that their pharaohs were gods, they entrusted their rulers with many responsibilities. Protection was at the top of the list. The pharaoh directed the army in case of a foreign threat or an internal conflict. All laws were enacted at the discretion of the pharaoh. Each farmer paid taxes in the form of grains, which were stored in the pharaohs warehouses. This grain was used to feed the people in the event of a famine.  The Chain of Command  No single person could manage all these duties without assistance. The pharaoh appointed a chief minister called a vizier as a supervisor. The vizier ensured that taxes were collected.  Working with the vizier were scribes who kept government records. These high-level employees had mastered a rare skill in ancient Egypt  they could read and write.  Noble Aims  Right below the pharaoh in status were powerful nobles and priests. Only nobles could hold government posts; in these positions they profited from tributes paid to the pharaoh. Priests were responsible for pleasing the gods.  Nobles enjoyed great status and also grew wealthy from donations to the gods. All Egyptiansfrom pharaohs to farmersgave gifts to the gods.  Soldier On  Soldiers fought in wars or quelled domestic uprisings. During long periods of peace, soldiers also supervised the peasants, farmers, and slaves who were involved in building such structures as pyramids and palaces.  Skilled workers such as physicians and craftsmen/women made up the middle class. Craftsmen made and sold jewelry, pottery, papyrus products, tools, and other useful things.  Naturally, there were people needed to buy goods from artisans and traders. These were the merchants and storekeepers who sold these goods to the public.  The Bottom of the Heap  At the bottom of the social structure were slaves and farmers. Slavery became the fate of those captured as prisoners of war. In addition to being forced to work on building projects, slaves toiled at the discretion of the pharaoh or nobles.  Farmers tended the fields, raised animals, kept canals and reservoirs in good order, worked in the stone quarries, and built the royal monuments. Farmers paid taxes that could amount to as much as 60% of their yearly harvestthats a lot of hay!  Social mobility was not impossible. A small number of peasants and farmers moved up the economic ladder. Families saved money to send their sons to village schools to learn trades. These schools were run by priests or by artisans. Boys who learned to read and write could become scribes, then go on to gain employment in the government. It was possible for a boy born on a farm to work his way up into the higher ranks of the government. Bureaucracy proved lucrative.\"\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Summary Quality: 1.2776761\n",
      "Enter a summary (type 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "#perform RNNS model\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "#DATASET\n",
    "summaries_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_train.csv\"\n",
    "summaries_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\summaries_test.csv\"\n",
    "prompts_train_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_train.csv\"\n",
    "prompts_test_path = \"C:\\\\Users\\\\nh013\\\\Desktop\\\\CommonLit - Evaluate Student Summaries\\\\prompts_test.csv\"\n",
    "\n",
    "summaries_train_df = pd.read_csv(summaries_train_path)\n",
    "summaries_test_df = pd.read_csv(summaries_test_path)\n",
    "prompts_train_df = pd.read_csv(prompts_train_path)\n",
    "prompts_test_df = pd.read_csv(prompts_test_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'content' and 'wording' COLUMNS ARE CONVERTED TO STRING\n",
    "summaries_train_df['content'] = summaries_train_df['content'].astype(str)\n",
    "summaries_train_df['wording'] = summaries_train_df['wording'].astype(str)\n",
    "\n",
    "#COMBINE TEXT DATA FROM SUMMARAISE AND PROMPTS DATAFRANE\n",
    "summaries_train_df['text'] = summaries_train_df['content'] + \" \" + summaries_train_df['wording']\n",
    "prompts_train_df['prompt_text'] = prompts_train_df['prompt_question'] + \" \" + prompts_train_df['prompt_title'] + \" \" + prompts_train_df['prompt_text']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FUNCTION\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # CONVERT TO LOWERCASE\n",
    "    text = text.lower()\n",
    "    \n",
    "    #REMOVE URLS\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # PREMOVE SPECIAL CHERECTER AND PUNCTUATION\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # TOKENIZATION\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # REMOVE STOPWORDS\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPROCESSING FOR  summaries_train_df\n",
    "summaries_train_df['preprocessed_text'] = summaries_train_df['text'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_content'] = summaries_train_df['content'].apply(preprocess_text)\n",
    "summaries_train_df['preprocessed_wording'] = summaries_train_df['wording'].apply(preprocess_text)\n",
    "\n",
    "# PREPROCESSING FOR prompts_train_df\n",
    "prompts_train_df['preprocessed_prompt_text'] = prompts_train_df['prompt_text'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_question'] = prompts_train_df['prompt_question'].apply(preprocess_text)\n",
    "prompts_train_df['preprocessed_prompt_title'] = prompts_train_df['prompt_title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# TOKENIZATION, STEMMING, AND LEMMATIZATION\n",
    "summaries_train_df['tokens'] = summaries_train_df['preprocessed_text'].apply(word_tokenize)\n",
    "summaries_train_df['stemmed_tokens'] = summaries_train_df['tokens'].apply(apply_stemming)\n",
    "summaries_train_df['lemmatized_tokens'] = summaries_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "prompts_train_df['tokens'] = prompts_train_df['preprocessed_prompt_text'].apply(word_tokenize)\n",
    "prompts_train_df['stemmed_tokens'] = prompts_train_df['tokens'].apply(apply_stemming)\n",
    "prompts_train_df['lemmatized_tokens'] = prompts_train_df['tokens'].apply(apply_lemmatization)\n",
    "\n",
    "\n",
    "# PART OF SPEECH TAGGING\n",
    "def apply_pos_tagging(tokens):\n",
    "    return pos_tag(tokens)\n",
    "\n",
    "summaries_train_df['pos_tags'] = summaries_train_df['tokens'].apply(apply_pos_tagging)\n",
    "prompts_train_df['pos_tags'] = prompts_train_df['tokens'].apply(apply_pos_tagging)\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF VECTORIZATION\n",
    "def apply_tfidf(tokens):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(tokens)])\n",
    "    return tfidf_matrix\n",
    "\n",
    "summaries_train_df['tfidf_matrix'] = summaries_train_df['lemmatized_tokens'].apply(apply_tfidf)\n",
    "prompts_train_df['tfidf_matrix'] = prompts_train_df['lemmatized_tokens'].apply(apply_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT 'content' and 'wording' COLUMNS TO NUMERIC\n",
    "summaries_train_df['content'] = pd.to_numeric(summaries_train_df['content'], errors='coerce')\n",
    "summaries_train_df['wording'] = pd.to_numeric(summaries_train_df['wording'], errors='coerce')\n",
    "\n",
    "# DROP NAN VALUES in 'content' and 'wording' columns\n",
    "summaries_train_df = summaries_train_df.dropna(subset=['content', 'wording'])\n",
    "\n",
    "\n",
    "# CONCATENATE THE  numeric 'content' and 'wording' COLUMNS\n",
    "summaries_train_df['text'] = summaries_train_df['content'] + summaries_train_df['wording']\n",
    "\n",
    "# SPLIT DATA \n",
    "X = summaries_train_df['text'].astype(str) + \" \" + summaries_train_df['preprocessed_content'].astype(str) + \" \" + summaries_train_df['preprocessed_wording'].astype(str)\n",
    "\n",
    "y = summaries_train_df['wording'].values  # Convert target variable to NumPy array for regression\n",
    "\n",
    "\n",
    "# TOKENIZE THE TEXT \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, padding='post')\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND VALIDATION SET\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# BUILD MODEL \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_padded.shape[1]))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=64))\n",
    "model.add(Dense(units=1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# TRAIN THE MODEL\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)\n",
    "\n",
    "# EVALUATE \n",
    "loss = model.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss:\", loss)\n",
    "\n",
    "# PREDICTIONS \n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "\n",
    "# LET'S CREATE INTERECTION LOOPS\n",
    "def preprocess_user_input(text):\n",
    "    \n",
    "    # PREPROCESS USER INPUT TEXT\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    preprocessed_tokens = word_tokenize(preprocessed_text)\n",
    "    stemmed_tokens = apply_stemming(preprocessed_tokens)\n",
    "    lemmatized_tokens = apply_lemmatization(preprocessed_tokens)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def assess_summary_quality(user_input):\n",
    "    \n",
    "    \n",
    "    #PREPROCESS USER INPUT\n",
    "    preprocessed_input = preprocess_user_input(user_input)\n",
    "    \n",
    "    # TOKENIZE AND PAD THE INPUT\n",
    "    input_sequences = tokenizer.texts_to_sequences([preprocessed_input])\n",
    "    padded_input = pad_sequences(input_sequences, padding='post', maxlen=X_padded.shape[1])\n",
    "    \n",
    "    \n",
    "    # MAKE PREDICTION\n",
    "    predicted_quality = model.predict(padded_input)\n",
    "    \n",
    "    return predicted_quality[0][0]\n",
    "\n",
    "# INTERECTION LOOP  EXECUTE\n",
    "while True:\n",
    "    user_input = input(\"Enter a summary (type 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    predicted_quality = assess_summary_quality(user_input)\n",
    "    print(\"Predicted Summary Quality:\", predicted_quality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
